{
  # Spark relation config
  spark: {
    app: {
        name: LPA
        # spark.app.partitionNum
        partitionNum: 10
    }
    master: "local[4]"
  }
  data: {
    # data source. optional of nebula,csv,json
    source: nebula
    # data sink, means the algorithm result will be write into this sink. optional of nebula,csv
    sink: csv
    # if your algorithm needs weight
    hasWeight: false
  }
  # Nebula Graph relation config
  nebula: {
    # algo's data source from Nebula. If data.source is nebula, then this nebula.read config can be valid.
    read: {
        # Nebula metad server address, multiple addresses are split by English comma
        metaAddress: "192.168.100.46:9559"
        # Nebula space
        space: dwd_graph_ly
        # Nebula edge types, multiple labels means that data from multiple edges will union together
        labels: ["edge_feature"]

        # Nebula edge property name for each edge type, this property will be as weight col for algorithm.
        # Make sure the weightCols are corresponding to labels.
        weightCols: []
    }

    # algo result sink into Nebula. If data.sink is nebula, then this nebula.write config can be valid.
    write: {
        # Nebula graphd server address， multiple addresses are split by English comma
        graphAddress: "192.168.100.46:9669"
        # Nebula metad server address, multiple addresses are split by English comma
        metaAddress: "192.168.100.46:9559"
        user: root
        password: nebula
        # Nebula space name
        space: dwd_graph_ly
        # Nebula tag name, the algorithm result will be write into this tag
        tag: pagerank
        # algorithm result is insert into new tag or update to original tag. type: insert/update
        type: insert
    }
  }
  local: {
      # algo's data source from Nebula. If data.source is csv or json, then this local.read can be valid.
      read:{
          filePath: "file:///tmp/algo_edge.csv"
          # srcId column
          srcId:"_c0"
          # dstId column
          dstId:"_c1"
          # weight column
          #weight: "col3"
          # if csv file has header
          header: false
          # csv file's delimiter
          delimiter:","
      }

      # 算法结果写入本地文件。 如果 data.sink 是 csv 或 text，那么这个 local.write 可以是有效的。
      write:{
          resultPath: /I/Desktop/alg_nebula/src/main/resources/pk
      }
  }
  algorithm: {
    # 你要执行的算法，从 [pagerank, louvain, connectedcomponent,
    # labelpropagation, shortestpaths, degreestatic, kcore, strongconnectedcomponent, trianglecount,
    # betweenness, graphtriangleCount, clusteringcoefficient, bfs, hanp, closeness, jaccard, node2vec]  中选择一个
    executeAlgo: pagerank

    louvain: {
        maxIter: 20
        internalIter: 10
        tol: 0.8
   }
   scc: {
        maxIter: 20
   }

   pagerank: {
       maxIter: 10
       resetProb: 0.15
   }

 }


}